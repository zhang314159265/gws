# TODO
- filetune gptoss-120b
- estimate the parameters.
- check reference implementation <===
  - implement it myself.

- serve gptoss with vllm

- try gptoss serving
  - it's response uses a format called harmony

- check chat.py

- benchmarking
  - [](https://www.baseten.co/blog/sota-performance-for-gpt-oss-120b-on-nvidia-gpus/#step-1-running-first-inference)
  - [] (https://www.reddit.com/r/LocalLLaMA/comments/1mijza6/vllm_latencythroughput_benchmarks_for_gptoss120b/)
  - [] (https://www.clarifai.com/blog/comparing-sglang-vllm-and-tensorrt-llm-with-gpt-oss-120b)


# Reference
- [openai announcement](https://openai.com/index/introducing-gpt-oss/)
- [huggingface model card for gptoss 20b](https://huggingface.co/openai/gpt-oss-20b)
- [gptoss github repo](https://github.com/openai/gpt-oss)
